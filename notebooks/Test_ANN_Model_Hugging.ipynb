{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WilmarAl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>emotion</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@erreborda</td>\n",
       "      <td>termine bien abrumado después de hoy</td>\n",
       "      <td>Jan 6, 2024 · 2:53 AM UTC</td>\n",
       "      <td>overwhelmed</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@shpiderduck</td>\n",
       "      <td>me siento abrumado❤</td>\n",
       "      <td>Jan 6, 2024 · 2:35 AM UTC</td>\n",
       "      <td>overwhelmed</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Alex_R_art</td>\n",
       "      <td>Me siento un poco abrumado por la cantidad de ...</td>\n",
       "      <td>Jan 6, 2024 · 12:20 AM UTC</td>\n",
       "      <td>overwhelmed</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@anggelinaa97</td>\n",
       "      <td>Salvador la única persona que no la ha abrumad...</td>\n",
       "      <td>Jan 5, 2024 · 10:38 PM UTC</td>\n",
       "      <td>overwhelmed</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@diegoreyesvqz</td>\n",
       "      <td>Denme un helado o algo que ando full abrumado.</td>\n",
       "      <td>Jan 5, 2024 · 8:38 PM UTC</td>\n",
       "      <td>overwhelmed</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user                                               text  \\\n",
       "0      @erreborda               termine bien abrumado después de hoy   \n",
       "1    @shpiderduck                                me siento abrumado❤   \n",
       "2     @Alex_R_art  Me siento un poco abrumado por la cantidad de ...   \n",
       "3   @anggelinaa97  Salvador la única persona que no la ha abrumad...   \n",
       "4  @diegoreyesvqz     Denme un helado o algo que ando full abrumado.   \n",
       "\n",
       "                         date      emotion sentiment  \n",
       "0   Jan 6, 2024 · 2:53 AM UTC  overwhelmed    scared  \n",
       "1   Jan 6, 2024 · 2:35 AM UTC  overwhelmed    scared  \n",
       "2  Jan 6, 2024 · 12:20 AM UTC  overwhelmed    scared  \n",
       "3  Jan 5, 2024 · 10:38 PM UTC  overwhelmed    scared  \n",
       "4   Jan 5, 2024 · 8:38 PM UTC  overwhelmed    scared  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../notebooks/data/sentiment_analysis_dataset.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento (Limpieza, Tokenización, Stopwords y Lematización)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traducción de las etiquetas a español\n",
    "translation = {\n",
    "    'joyful': 'Alegre',\n",
    "    'daring': 'Osado',\n",
    "    'optimistic': 'Optimista',\n",
    "    'playful': 'Jugueton',\n",
    "    'powerful': 'Poderoso',\n",
    "    'surprised': 'Sorprendido',\n",
    "    'successful': 'Exitoso',\n",
    "    'confident': 'Confiado',\n",
    "    'peaceful': 'Tranquilo',\n",
    "    'secure': 'Seguro',\n",
    "    'thankful': 'Agradecido',\n",
    "    'loving': 'Amoroso',\n",
    "    'relaxed': 'Relajado',\n",
    "    'responsive': 'Sensible',\n",
    "    'sad': 'Triste',\n",
    "    'sleepy': 'Adormilado',\n",
    "    'isolated': 'Aislado',\n",
    "    'stupid': 'Estupido',\n",
    "    'mad': 'Histerico',\n",
    "    'distant': 'Distante',\n",
    "    'frustrated': 'Frustrado',\n",
    "    'irritated': 'Irritado',\n",
    "    'jealous': 'Celoso',\n",
    "    'scared': 'Asustado',\n",
    "    'embarrassed': 'Avergonzado',\n",
    "    'overwhelmed': 'Agobiado',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>emotion</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@erreborda</td>\n",
       "      <td>termine bien abrumado después de hoy</td>\n",
       "      <td>Jan 6, 2024 · 2:53 AM UTC</td>\n",
       "      <td>Agobiado</td>\n",
       "      <td>Asustado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@shpiderduck</td>\n",
       "      <td>me siento abrumado❤</td>\n",
       "      <td>Jan 6, 2024 · 2:35 AM UTC</td>\n",
       "      <td>Agobiado</td>\n",
       "      <td>Asustado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Alex_R_art</td>\n",
       "      <td>Me siento un poco abrumado por la cantidad de ...</td>\n",
       "      <td>Jan 6, 2024 · 12:20 AM UTC</td>\n",
       "      <td>Agobiado</td>\n",
       "      <td>Asustado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@anggelinaa97</td>\n",
       "      <td>Salvador la única persona que no la ha abrumad...</td>\n",
       "      <td>Jan 5, 2024 · 10:38 PM UTC</td>\n",
       "      <td>Agobiado</td>\n",
       "      <td>Asustado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@diegoreyesvqz</td>\n",
       "      <td>Denme un helado o algo que ando full abrumado.</td>\n",
       "      <td>Jan 5, 2024 · 8:38 PM UTC</td>\n",
       "      <td>Agobiado</td>\n",
       "      <td>Asustado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user                                               text  \\\n",
       "0      @erreborda               termine bien abrumado después de hoy   \n",
       "1    @shpiderduck                                me siento abrumado❤   \n",
       "2     @Alex_R_art  Me siento un poco abrumado por la cantidad de ...   \n",
       "3   @anggelinaa97  Salvador la única persona que no la ha abrumad...   \n",
       "4  @diegoreyesvqz     Denme un helado o algo que ando full abrumado.   \n",
       "\n",
       "                         date   emotion sentiment  \n",
       "0   Jan 6, 2024 · 2:53 AM UTC  Agobiado  Asustado  \n",
       "1   Jan 6, 2024 · 2:35 AM UTC  Agobiado  Asustado  \n",
       "2  Jan 6, 2024 · 12:20 AM UTC  Agobiado  Asustado  \n",
       "3  Jan 5, 2024 · 10:38 PM UTC  Agobiado  Asustado  \n",
       "4   Jan 5, 2024 · 8:38 PM UTC  Agobiado  Asustado  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['emotion', 'sentiment']] = data[['emotion', 'sentiment']].replace(to_replace = translation)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_map ={\n",
    "#     'Alegre': 1, # Sentimiento 1\n",
    "#     'Osado': 1,\n",
    "#     'Optimista': 2,\n",
    "#     'Jugueton': 3,\n",
    "#     'Poderoso': 2, # Sentimiento 2\n",
    "#     'Sorprendido': 4,\n",
    "#     'Exitoso': 5, \n",
    "#     'Confiado': 6, \n",
    "#     'Tranquilo': 3, # Sentimiento 3\n",
    "#     'Seguro': 7,\n",
    "#     'Agradecido': 8,\n",
    "#     'Amoroso': 9,\n",
    "#     'Relajado': 10,\n",
    "#     'Sensible': 11,\n",
    "#     'Triste': 4, # Sentimiento 4\n",
    "#     'Adormilado': 12,\n",
    "#     'Aislado': 13,\n",
    "#     'Estupido': 14,\n",
    "#     'Histerico': 5, # Sentimiento 5\n",
    "#     'Distante': 15,\n",
    "#     'Frustrado': 16,\n",
    "#     'Irritado': 17,\n",
    "#     'Celoso': 18,\n",
    "#     'Asustado': 6, # Sentimiento 6\n",
    "#     'Avergonzado': 19,\n",
    "#     'Agobiado': 20,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotions = ['Osado', 'Optimista', 'Jugueton', 'Sorprendido', 'Exitoso', 'Confiado', 'Seguro', 'Agradecido', 'Amoroso', 'Relajado', 'Sensible', \n",
    "#            'Adormilado', 'Aislado', 'Histerico', 'Distante', 'Frustrado', 'Irritado', 'Celoso', 'Avergonzado', 'Agobiado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reversed_map = {value: sentiment for sentiment, value in target_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_map = {\n",
    "    'Osado': 'Felicidad',\n",
    "    'Optimista': 'Felicidad',\n",
    "    'Jugueton': 'Felicidad',\n",
    "    'Sorprendido': 'Empoderado',\n",
    "    'Exitoso': 'Empoderado',\n",
    "    'Confiado': 'Empoderado',\n",
    "    'Seguro': 'Paz',\n",
    "    'Agradecido': 'Paz',\n",
    "    'Amoroso': 'Paz',\n",
    "    'Relajado': 'Paz',\n",
    "    'Sensible': 'Paz',\n",
    "    'Adormilado': 'Tristeza',\n",
    "    'Aislado': 'Tristeza',\n",
    "    'Histerico': 'Tristeza',\n",
    "    'Distante': 'Furia',\n",
    "    'Frustrado': 'Furia',\n",
    "    'Irritado': 'Furia',\n",
    "    'Celoso': 'Furia',\n",
    "    'Avergonzado': 'Miedo',\n",
    "    'Agobiado': 'Miedo',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['emotion'] = data['emotion'].map(target_map)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['text', 'emotion']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WilmarAl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WilmarAl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Cargar el modelo de spaCy para español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Definir stopwords en español\n",
    "stop_words = set(stopwords.words('spanish')) | STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    text = text.lower()\n",
    "    text = text.replace('á','a').replace('é','e')\n",
    "    text = text.replace('í','i').replace('ó','o')\n",
    "    text = text.replace('ú','u').replace('$','')\n",
    "    text = text.replace('—','').replace('-',' ')\n",
    "    text = text.replace('%','').replace('&','')\n",
    "    text = text.replace('\\n',' ').replace('\\t',' ')\n",
    "    text = text.replace(\"'\",\"\").replace('\"','')\n",
    "    text = text.replace(',','').replace('.','')\n",
    "    text = text.replace(';','').replace(':','')\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # Eliminar menciones\n",
    "    text = re.sub(r'#[A-Za-z0-9_]+', '', text)  # Eliminar hashtags\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)  # Eliminar URLs\n",
    "    text = re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚñÑ ]', '', text)  # Eliminar caracteres especiales y emojis\n",
    "\n",
    "    #doc = nlp(text)\n",
    "    \n",
    "    # Eliminar stopwords y lematizar\n",
    "    #palabras_procesadas = [token.lemma_ for token in doc if token.text.lower() not in stop_words and not token.is_punct]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def text_to_vector(tokens, model):\n",
    "    # Obtener el vector promedio de todas las palabras en el texto\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)  # Promedio de los vectores\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Si no hay palabras conocidas, devolver un vector de ceros\n",
    "\n",
    "def vectorize_text_column(df, text_column, model_name='dccuchile/bert-base-spanish-wwm-cased', max_length=128):\n",
    "    \"\"\"\n",
    "    Vectoriza una columna de texto de un DataFrame usando un modelo preentrenado de Hugging Face.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene la columna de texto.\n",
    "    - text_column: Nombre de la columna de texto a vectorizar.\n",
    "    - model_name: Nombre del modelo preentrenado de Hugging Face (por defecto: BETO para español).\n",
    "    - max_length: Longitud máxima de los textos (por defecto: 128).\n",
    "\n",
    "    Retorna:\n",
    "    - embeddings: Array de numpy con los embeddings de los textos.\n",
    "    \"\"\"\n",
    "    # 1. Cargar tokenizer y modelo preentrenado\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # 2. Tokenizar los textos de la columna\n",
    "    texts = df[text_column].tolist()\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "\n",
    "    # 3. Extraer embeddings\n",
    "    with torch.no_grad():  # Desactiva el cálculo de gradientes para mayor eficiencia\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 4. Obtener el embedding de la oración (promedio de los embeddings de los tokens)\n",
    "    sentence_embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>termine bien abrumado despues de hoy</td>\n",
       "      <td>Agobiado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me siento abrumado</td>\n",
       "      <td>Agobiado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>me siento un poco abrumado por la cantidad de ...</td>\n",
       "      <td>Agobiado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>salvador la unica persona que no la ha abrumad...</td>\n",
       "      <td>Agobiado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denme un helado o algo que ando full abrumado</td>\n",
       "      <td>Agobiado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   emotion\n",
       "0               termine bien abrumado despues de hoy  Agobiado\n",
       "1                                 me siento abrumado  Agobiado\n",
       "2  me siento un poco abrumado por la cantidad de ...  Agobiado\n",
       "3  salvador la unica persona que no la ha abrumad...  Agobiado\n",
       "4      denme un helado o algo que ando full abrumado  Agobiado"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar la columna de texto\n",
    "embeddings = vectorize_text_column(df, text_column='text')\n",
    "\n",
    "# Almacenar los embeddings como una lista en una nueva columna\n",
    "df['embeddings'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar la columna de texto\n",
    "# embeddings = vectorize_text_column(df, text_column='emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>termine bien abrumado despues de hoy</td>\n",
       "      <td>Agobiado</td>\n",
       "      <td>[0.07573962211608887, -0.3575446903705597, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me siento abrumado</td>\n",
       "      <td>Agobiado</td>\n",
       "      <td>[-0.3122999370098114, -0.301361620426178, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>me siento un poco abrumado por la cantidad de ...</td>\n",
       "      <td>Agobiado</td>\n",
       "      <td>[-0.1711559295654297, -0.18406972289085388, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>salvador la unica persona que no la ha abrumad...</td>\n",
       "      <td>Agobiado</td>\n",
       "      <td>[-0.08535804599523544, 0.13760820031166077, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denme un helado o algo que ando full abrumado</td>\n",
       "      <td>Agobiado</td>\n",
       "      <td>[-0.1372677981853485, -0.2031136155128479, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   emotion  \\\n",
       "0               termine bien abrumado despues de hoy  Agobiado   \n",
       "1                                 me siento abrumado  Agobiado   \n",
       "2  me siento un poco abrumado por la cantidad de ...  Agobiado   \n",
       "3  salvador la unica persona que no la ha abrumad...  Agobiado   \n",
       "4      denme un helado o algo que ando full abrumado  Agobiado   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.07573962211608887, -0.3575446903705597, -0....  \n",
       "1  [-0.3122999370098114, -0.301361620426178, -0.0...  \n",
       "2  [-0.1711559295654297, -0.18406972289085388, -0...  \n",
       "3  [-0.08535804599523544, 0.13760820031166077, 0....  \n",
       "4  [-0.1372677981853485, -0.2031136155128479, -0....  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(df['embeddings'].values).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = df['emotion']  # Etiquetas (emotion)\n",
    "y = label_encoder.fit_transform(df['emotion'])  # Convierte las etiquetas a números\n",
    "num_classes = len(label_encoder.classes_)  # Número de clases\n",
    "# y_encoded = label_encoder.fit_transform(y)  # Convertir etiquetas a números\n",
    "# y_categorical = to_categorical(y_encoded)   # Convertir a one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los datos a tensores de PyTorch\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)  # Usar torch.long para índices de clase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tensor, y_tensor, test_size=0.15, random_state=42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y desempeño de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una red neuronal simple para clasificación multiclase\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 160)\n",
    "        self.dropout = nn.Dropout(0.3)  # Añade dropout en las capas\n",
    "        self.fc2 = nn.Linear(160, 80)\n",
    "        self.dropout = nn.Dropout(0.3)  # Añade dropout en las capas\n",
    "        self.fc3 = nn.Linear(80, 40)\n",
    "        self.dropout = nn.Dropout(0.3)  # Añade dropout en las capas\n",
    "        self.fc4 = nn.Linear(40, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.6, 3.6, 3.6, 3.7])  # Ajusta los pesos según el desbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataLoader para manejar batches\n",
    "batch_size = 32  # Ajusta el tamaño del batch\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo\n",
    "input_size = X_train.shape[1]  # Dimensión del embedding\n",
    "model = DeepNN(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inicializar el modelo\n",
    "# batch_size = 64  # Ajusta el tamaño del batch\n",
    "learning_rate = 0.01\n",
    "# num_epochs = 20\n",
    "# input_size = X_train.shape[1]  # Dimensión del embedding\n",
    "# model = DeepNN(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)  # Para clasificación multiclase\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 3.0005\n",
      "Epoch 2/40, Loss: 2.9641\n",
      "Epoch 3/40, Loss: 2.8421\n",
      "Epoch 4/40, Loss: 2.7426\n",
      "Epoch 5/40, Loss: 2.6208\n",
      "Epoch 6/40, Loss: 2.5440\n",
      "Epoch 7/40, Loss: 2.5262\n",
      "Epoch 8/40, Loss: 2.4313\n",
      "Epoch 9/40, Loss: 2.3602\n",
      "Epoch 10/40, Loss: 2.3010\n",
      "Epoch 11/40, Loss: 2.2519\n",
      "Epoch 12/40, Loss: 2.2619\n",
      "Epoch 13/40, Loss: 2.1675\n",
      "Epoch 14/40, Loss: 2.1342\n",
      "Epoch 15/40, Loss: 2.0667\n",
      "Epoch 16/40, Loss: 2.0390\n",
      "Epoch 17/40, Loss: 1.9784\n",
      "Epoch 18/40, Loss: 1.9782\n",
      "Epoch 19/40, Loss: 1.9846\n",
      "Epoch 20/40, Loss: 1.9123\n",
      "Epoch 21/40, Loss: 1.8797\n",
      "Epoch 22/40, Loss: 1.8173\n",
      "Epoch 23/40, Loss: 1.7965\n",
      "Epoch 24/40, Loss: 1.7524\n",
      "Epoch 25/40, Loss: 1.7483\n",
      "Epoch 26/40, Loss: 1.8236\n",
      "Epoch 27/40, Loss: 1.7801\n",
      "Epoch 28/40, Loss: 1.6713\n",
      "Epoch 29/40, Loss: 1.6737\n",
      "Epoch 30/40, Loss: 1.6505\n",
      "Epoch 31/40, Loss: 1.6026\n",
      "Epoch 32/40, Loss: 1.5953\n",
      "Epoch 33/40, Loss: 1.6150\n",
      "Epoch 34/40, Loss: 1.6278\n",
      "Epoch 35/40, Loss: 1.5870\n",
      "Epoch 36/40, Loss: 1.5178\n",
      "Epoch 37/40, Loss: 1.6350\n",
      "Epoch 38/40, Loss: 1.5555\n",
      "Epoch 39/40, Loss: 1.5233\n",
      "Epoch 40/40, Loss: 1.4900\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "num_epochs = 40  # Ajusta el número de épocas\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Poner el modelo en modo entrenamiento\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Imprimir la pérdida promedio por época\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Entrenar el modelo\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train() \n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X_train)\n",
    "#     loss = criterion(outputs, Y_train)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.02%\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "model.eval()  # Poner el modelo en modo evaluación\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)  # Obtener la clase predicha\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluar el modelo\n",
    "# model.eval()  # Poner el modelo en modo evaluación\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_test)\n",
    "#     _, predicted = torch.max(outputs, 1)  # Obtener la clase predicha\n",
    "#     accuracy = (predicted == Y_test).float().mean()\n",
    "#     print(f'Accuracy: {accuracy.item() * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Texto de entrada\n",
    "# user_text = 'La verdad extraño la persona que solías ser, que me recordaba que debo descansar sin sentirme culpable, que se molestó en conocer mi lenguaje corporal y me hizo sentir que quererme no era difícil. Te lo agradezco, pero te llevaste mi parte más vulnerable y ahora no sé qué hacer.'\n",
    "\n",
    "# # 1. Preprocesar el texto de entrada\n",
    "# user_tokens = preprocess_text(user_text)  # Tokenización\n",
    "# user_vector = text_to_vector(user_tokens, word2vec_model)  # Convertir a vector\n",
    "\n",
    "# # 2. Convertir el vector en un formato compatible con el modelo\n",
    "# user_vector = np.array([user_vector])  # Añadir una dimensión extra (batch size = 1)\n",
    "\n",
    "# # 3. Hacer la predicción con la red neuronal\n",
    "# prediction = model_ann.predict(user_vector)  # Obtener las probabilidades de cada clase\n",
    "# predicted_class_index = np.argmax(prediction, axis=1)  # Obtener la clase predicha\n",
    "\n",
    "# # 4. Convertir el índice de la clase predicha a la etiqueta original\n",
    "# predicted_class = label_encoder.inverse_transform(predicted_class_index)\n",
    "\n",
    "# # 5. Mostrar el resultado\n",
    "# print(f\"Texto: {user_text}\")\n",
    "# print(f\"Clase predicha: {predicted_class[0]}\")\n",
    "# print(f\"Probabilidades por clase: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
